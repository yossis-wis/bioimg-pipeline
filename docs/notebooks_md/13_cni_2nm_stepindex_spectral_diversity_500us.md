<!--
AUTO-GENERATED FILE. DO NOT EDIT DIRECTLY.
Source: notebooks/13_cni_2nm_stepindex_spectral_diversity_500us.py
Generated by: scripts/export_notebooks_markdown.py
-->

# CNI FC-640-class (2–3 nm linewidth) + 3 m step-index MMF: will **spectral diversity alone** make 500 µs work?

This notebook is an intentionally *engineering-first* “chapter” analysis of the specific, practical question:

> You have a quote for a **fiber-coupled 640 nm source** with **spectral linewidth ~2–3 nm** (CNI FC-640-class).
> You plan to deliver it through a **3 m, step-index, 400 µm core multimode fiber**.
> You want to know whether **spectral diversity by itself** (no scramblers, no diffusers, no moving parts,
> no polarization diversity, no angle diversity) can yield **sufficiently homogeneous excitation** for:
>
> - ROI: **30 µm × 30 µm** (100× objective)
> - exposure: **500 µs**
> - irradiance: **10–30 kW/cm²**

What we do here
--------------
1. Translate the fiber spec into a **speckle spectral correlation width** $`\Delta\lambda_c`$.
2. Translate the vendor “2–3 nm” statement into several **plausible spectral scenarios**
   (continuous envelope vs a comb of discrete longitudinal modes vs “linewidth” that is only time-averaged).
3. For each scenario, compute **two outputs**:
   - **$C$**, a speckle-contrast metric inside the ROI (homogeneity proxy).
   - a **Slice0 confusion-matrix proxy** (TP/FP/FN plus a finite-TN “decoy site” proxy),
     measured on a synthetic sparse-emitter frame.
4. Show representative **component speckle fields** and the **summed/averaged field** for each scenario.

Scope / constraints (per your request)
-------------------------------------
- Step-index MMF only (no graded-index discussion).
- Spectral diversity only.
  - No vibrating fiber, no mechanical scrambler, no diffuser, no angle diversity, no polarization diversity.
- We care about the **500 µs simultaneity** question:
  - Are all spectral components present *simultaneously* during a single 500 µs exposure?

**Important realism warning**
----------------------------
Vendor “spectral linewidth ~2–3 nm” can mean different things:

- (Best case) a truly **multi-longitudinal-mode** source emitting many lines at once.
- (Worst case) a **narrow** source that **mode-hops** over time so the *OSA trace* looks broad,
  but at any instant (500 µs) only ~1 line is present.

Because of this ambiguity, the correct output of this notebook is not a single number.
It is a **scenario map**: scenario → (C, confusion matrix).

## 0) Imports + repo plumbing

<details>
<summary>Code cell 1</summary>

```python
from __future__ import annotations

import math
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Iterable, Optional

import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# If we are in a notebook, prefer inline backend.
if "ipykernel" in sys.modules:
    try:
        if matplotlib.get_backend().lower() == "agg":
            matplotlib.use("module://matplotlib_inline.backend_inline", force=True)
    except Exception:
        pass


def find_repo_root(start: Path) -> Path:
    """Find repo root by walking upward until we see (src/, environment.yml)."""

    p = start.resolve()
    for parent in [p, *p.parents]:
        if (parent / "src").is_dir() and (parent / "environment.yml").exists():
            return parent
    return p


REPO_ROOT = find_repo_root(Path.cwd())
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from src.excitation_speckle_sim import (  # noqa: E402
    square_roi_mask,
    simulate_excitation_speckle_field,
)
from src.mmf_fiber_speckle import (  # noqa: E402
    MultimodeFiber,
    max_guided_meridional_ray_angle_rad,
    optical_path_spread_geometric_m,
    speckle_spectral_corr_width_nm,
)
from src.speckle_weighting import (  # noqa: E402
    effective_n_from_weights,
    gaussian_spectrum_bins,
    speckle_contrast_from_weights,
    uniform_top_hat_spectrum_bins,
)
from src.slice0_kernel import Slice0Params, detect_spots  # noqa: E402

# Make plots a bit larger by default.
plt.rcParams.update({"figure.figsize": (8.0, 5.0), "figure.dpi": 120})
```

</details>

## 1) Configuration (all user-editable)

We separate **what comes from the vendor** (spec-sheet-type values) from
**what comes from physical materials** (base units).

<details>
<summary>Code cell 2</summary>

```python
# ---------------- Vendor-like inputs ----------------
lambda0_nm = 640.0
linewidth_fwhm_nm_nominal = 2.5  # interpret "2–3 nm" as a nominal FWHM

# From the quote / target requirement
roi_um = 30.0
exposure_us = 500.0

# ---------------- Fiber (step-index) ----------------
fiber = MultimodeFiber(
    core_diameter_um=400.0,
    na=0.22,
    length_m=3.0,
    n_core=1.46,
    modal_delay_scale=1.0,  # step-index assumption
)

# ---------------- Microscope sampling (for Slice0 sim) ----------------
M_obj = 100.0
camera_pixel_um = 6.5
# sample-plane pixel pitch
dx_um = camera_pixel_um / M_obj

# Simulation grid
# 512 is faster but leaves very little margin around a 30 µm ROI at 65 nm/px.
# 640 gives a more comfortable margin while still being reasonably fast.
N_grid = 640

# Inner ROI margin for C computation (exclude field-stop edge roll-off)
inner_margin_um = 2.0

# Effective illumination NA at the sample (BFP underfill knob).
# This is NOT the fiber NA. It is set by your relay optics and underfill fraction.
# We'll keep a single baseline and then do a small sweep later.
NA_obj = 1.40
underfill_ratio = 0.10  # 0.10 means fill ~10% of the objective pupil diameter
NA_illum = underfill_ratio * NA_obj

# RNG seed used for representative visualization fields (separate from Monte Carlo below)
seed_vis = 0
```

</details>

### 1.1 Quick power sanity check (not the main topic)

Area of a 30 µm × 30 µm ROI is:

$$
A = (30\thinspace\mu\mathrm{m})^2 = 900\thinspace\mu\mathrm{m}^2 = 9\times 10^{-6}\thinspace\mathrm{cm}^2.
$$

So 10–30 kW/cm² corresponds to 90–270 mW at the sample.
A 2 W class source leaves plenty of margin for losses.

<details>
<summary>Code cell 3</summary>

```python
area_um2 = roi_um * roi_um
area_cm2 = area_um2 / 1e8
for irr_kw_cm2 in [10.0, 30.0]:
    p_w = (irr_kw_cm2 * 1e3) * area_cm2
    print(f"ROI {roi_um:.0f}×{roi_um:.0f} µm² @ {irr_kw_cm2:.0f} kW/cm² -> {p_w*1e3:.0f} mW at sample")
```

</details>

## 2) From **base fiber properties** to $`\Delta\lambda_c`$ (speckle spectral correlation width)

The core modeling idea is extremely simple:

- A step-index MMF supports rays/modes at different angles $\theta$.
- Different angles correspond to different **axial group delays** (different transit times) and thus different
  **optical path lengths**.
- If the source has finite linewidth, those different delays wash out coherence between modal contributions.

We compute (geometric optics):

$$
\theta_{\max} = \arcsin\left(\frac{\mathrm{NA}}{n_{\mathrm{core}}}\right)
$$

$$
\Delta\mathrm{OPL} = n_{\mathrm{core}}\,L\left(\frac{1}{\cos\theta_{\max}} - 1\right)
\approx \frac{\mathrm{NA}^2}{2n_{\mathrm{core}}}\,L
$$

and then the speckle spectral correlation width:

$$
\Delta\lambda_c \sim \frac{\lambda_0^2}{\Delta\mathrm{OPL}}.
$$

This gives an explicit dependency of the *homogeneity proxy* $C$ on base units.
In the equal-weight, independent-bin limit:

$$
N_\lambda \approx \frac{\Delta\lambda_{\mathrm{src}}}{\Delta\lambda_c},\qquad
C \sim \frac{1}{\sqrt{N_\lambda}}\sim \sqrt{\frac{\Delta\lambda_c}{\Delta\lambda_{\mathrm{src}}}}.
$$

Substituting the small-angle step-index approximation yields:

$$
C \sim \sqrt{\frac{2 n_{\mathrm{core}}\,\lambda_0^2}{L\,\mathrm{NA}^2\,\Delta\lambda_{\mathrm{src}}}}.
$$

That equation is the “explicit physical parameter” form:
$C$ depends on $n_{\mathrm{core}}$, NA, length, and linewidth.

<details>
<summary>Code cell 4</summary>

```python
# Base-parameter computations

theta_max_rad = max_guided_meridional_ray_angle_rad(na=fiber.na, n_core=fiber.n_core)
delta_opl_m = optical_path_spread_geometric_m(fiber)
dlam_c_nm = speckle_spectral_corr_width_nm(lambda0_nm=lambda0_nm, delta_opl_m=delta_opl_m)

# Also compute n_clad implied by NA (air outside): NA^2 = n_core^2 - n_clad^2
n_clad = math.sqrt(max(fiber.n_core * fiber.n_core - fiber.na * fiber.na, 0.0))

df_fiber = pd.DataFrame(
    [
        {
            "lambda0_nm": lambda0_nm,
            "L_m": fiber.length_m,
            "core_diameter_um": fiber.core_diameter_um,
            "n_core": fiber.n_core,
            "n_clad (from NA)": n_clad,
            "NA": fiber.na,
            "theta_max_deg": theta_max_rad * 180.0 / math.pi,
            "ΔOPL (mm)": delta_opl_m * 1e3,
            "Δλ_c (nm)": dlam_c_nm,
        }
    ]
)

df_fiber
```

</details>

### 2.1 What does $`\Delta\lambda_c`$ mean operationally?

- If two wavelengths differ by **much less** than $`\Delta\lambda_c`$, they generate **highly correlated speckle**.
  Their intensities do *not* average down much.
- If they differ by **much more** than $`\Delta\lambda_c`$, they generate **nearly independent speckle**.
  Their intensities average down like $C\sim 1/\sqrt{N}$.

With the 3 m, NA=0.22 step-index fiber, $`\Delta\lambda_c`$ is typically **tiny** (often ~0.01 nm scale).
That means even a “small” linewidth (2–3 nm) spans **hundreds** of correlation widths.

The catch is that this is only useful if the spectrum is **present simultaneously** within the exposure.

<details>
<summary>Code cell 5</summary>

```python
print(f"Computed speckle spectral correlation width: Δλ_c ≈ {dlam_c_nm:.4f} nm")
print(f"Nominal vendor linewidth: Δλ_src ≈ {linewidth_fwhm_nm_nominal:.2f} nm")
print(f"Span / corr_width ≈ {linewidth_fwhm_nm_nominal / dlam_c_nm:.0f} independent bins (upper bound)")
```

</details>

## 3) What does “spectral linewidth ~2–3 nm” *actually* mean?

Here are the main interpretations that matter for your **spectral diversity** approach.
These are not philosophical distinctions — they change $N_\lambda$ by orders of magnitude.

### Interpretation A: Continuous-ish spectrum (best case)
A broad envelope is present **simultaneously** (ASE-like, or a very dense comb).
Then $N_\lambda$ is limited mainly by the fiber’s $`\Delta\lambda_c`$.

### Interpretation B: Discrete longitudinal modes, all lasing simultaneously (common case)
A Fabry–Perot diode can lase on **many longitudinal modes** at once.
The OSA trace shows multiple spikes.
Then $N_\lambda$ is roughly the **number of spikes** (if each spike is narrower than $`\Delta\lambda_c`$),
unless the spikes are so dense that they fill the band.

### Interpretation C: “Linewidth” is time-averaged (worst case for *within-exposure* averaging)
If the laser mode-hops slowly, an OSA trace may show 2–3 nm wide output, but at any instant
there may be only ~1 mode (or a few).
In that case, within a **single 500 µs exposure**, you do *not* get large $N_\lambda$.
Instead you get time-varying speckle frame-to-frame (multiplicative noise), which is exactly what
you were trying to avoid.

Because you don’t yet have a time-resolved spectrum measurement for the CNI source, the correct move is:

- define a small set of **scenarios**,
- compute (C, confusion matrix) for each, and
- ask the vendor for just enough additional information to collapse the uncertainty.

## 4) Scenario definitions (spectrum → weights → $N_{\mathrm{eff}}$ → predicted $C$)

We represent the spectrum by a list of **spectral components** $\{w_i\}$ whose weights sum to 1.
If each component generates an independent speckle realization,
then for fully developed speckle the residual contrast is:

$$
C = \sqrt{\sum_i w_i^2} = \frac{1}{\sqrt{N_{\mathrm{eff}}}},\qquad
N_{\mathrm{eff}} = \frac{1}{\sum_i w_i^2}.
$$

This handles both:
- equal-power spikes (all $w_i=1/N$ → $C=1/\sqrt{N}$), and
- unequal powers (one line dominates → $C$ stays high).

We build a few scenarios that cover the plausible range for a “2–3 nm linewidth” vendor statement.

<details>
<summary>Code cell 6</summary>

```python
@dataclass(frozen=True)
class Scenario:
    name: str
    kind: str  # "single" | "spikes" | "top_hat" | "gaussian"
    # Spectrum parameters
    span_nm: Optional[float] = None  # for top-hat
    fwhm_nm: Optional[float] = None  # for gaussian
    n_spikes: Optional[int] = None
    spike_weights: Optional[np.ndarray] = None  # if provided, overrides equal weights
    notes: str = ""


def make_equal_spike_weights(n: int) -> np.ndarray:
    if n < 1:
        raise ValueError("n must be >= 1")
    return np.full(n, 1.0 / n, dtype=np.float64)


def make_skewed_spike_weights(n: int, dominant_frac: float) -> np.ndarray:
    """One dominant spike + flat remainder."""
    if n < 2:
        raise ValueError("n must be >= 2")
    if not (0.0 < dominant_frac < 1.0):
        raise ValueError("dominant_frac must be in (0,1)")
    rest = 1.0 - dominant_frac
    w = np.full(n, rest / (n - 1), dtype=np.float64)
    w[0] = dominant_frac
    return w


# A small set of scenarios spanning "optimistic" to "pessimistic" interpretations.
scenarios: list[Scenario] = [
    Scenario(
        name="C0: instantaneous single-line (worst case)",
        kind="single",
        notes=(
            "OSA may show 2–3 nm because of slow mode hopping, but within 500 µs the output is effectively single-mode."
        ),
    ),
    Scenario(
        name="C1: 10 spikes, equal power",
        kind="spikes",
        n_spikes=10,
        notes="Low-count Fabry–Perot case (or filtered).",
    ),
    Scenario(
        name="C2: 20 spikes, equal power",
        kind="spikes",
        n_spikes=20,
        notes=(
            "Plausible for a short-cavity diode where the 2–3 nm envelope contains ~10–30 longitudinal modes."
        ),
    ),
    Scenario(
        name="C3: 100 spikes, equal power",
        kind="spikes",
        n_spikes=100,
        notes=(
            "Optimistic discrete-line case (dense comb or longer cavity)."
        ),
    ),
    Scenario(
        name="C4: 20 spikes, one dominates 50%",
        kind="spikes",
        n_spikes=20,
        spike_weights=make_skewed_spike_weights(20, dominant_frac=0.50),
        notes="If most power is in one line, spectral diversity is much weaker than the FWHM number suggests.",
    ),
    Scenario(
        name="C5: top-hat continuum over 2.5 nm (upper bound)",
        kind="top_hat",
        span_nm=linewidth_fwhm_nm_nominal,
        notes=(
            "Best-case interpretation: spectrum is simultaneously present and roughly flat across 2–3 nm."
        ),
    ),
]


def scenario_weights_and_neff(
    sc: Scenario,
    *,
    lambda0_nm: float,
    corr_width_nm: float,
) -> tuple[np.ndarray, float, str]:
    """Return (weights, N_eff, label) for speckle averaging."""

    if sc.kind == "single":
        w = np.array([1.0], dtype=np.float64)
        return w, 1.0, "single"

    if sc.kind == "spikes":
        if sc.spike_weights is not None:
            w = np.asarray(sc.spike_weights, dtype=np.float64)
        else:
            if sc.n_spikes is None:
                raise ValueError("spikes scenario requires n_spikes")
            w = make_equal_spike_weights(int(sc.n_spikes))
        w = w / float(w.sum())
        neff = float(effective_n_from_weights(w))
        return w, neff, f"{len(w)} spikes"

    if sc.kind == "top_hat":
        if sc.span_nm is None:
            raise ValueError("top_hat scenario requires span_nm")
        bins = uniform_top_hat_spectrum_bins(lambda0_nm=lambda0_nm, span_nm=float(sc.span_nm), corr_width_nm=corr_width_nm)
        w = bins.weights
        neff = float(effective_n_from_weights(w))
        return w, neff, f"top-hat ({len(w)} bins)"

    if sc.kind == "gaussian":
        if sc.fwhm_nm is None:
            raise ValueError("gaussian scenario requires fwhm_nm")
        bins = gaussian_spectrum_bins(lambda0_nm=lambda0_nm, fwhm_nm=float(sc.fwhm_nm), corr_width_nm=corr_width_nm, n_std=2.0)
        w = bins.weights
        neff = float(effective_n_from_weights(w))
        return w, neff, f"gaussian ({len(w)} bins)"

    raise ValueError(f"Unknown scenario kind: {sc.kind}")


rows = []
for sc in scenarios:
    w, neff, label = scenario_weights_and_neff(sc, lambda0_nm=lambda0_nm, corr_width_nm=dlam_c_nm)
    C_pred = float(speckle_contrast_from_weights(w))
    rows.append(
        {
            "scenario": sc.name,
            "spectrum_model": label,
            "N_eff (from weights)": neff,
            "C_pred": C_pred,
            "notes": sc.notes,
        }
    )

df_scen = pd.DataFrame(rows)
df_scen
```

</details>

### 4.1 A practical “sanity line” for $C$

A common intuition anchor:

- $C=0.10$ means **10% RMS** intensity fluctuations (relative to mean).
- $C=0.05$ means **5% RMS**.

Many people will *visually* call something “homogeneous” somewhere in the 0.05–0.10 ballpark,
but **spot detection** can care about both:

- amplitude (contrast), and
- correlation length (speckle grain size vs PSF scale).

That’s why we compute both $C$ and a detector-level metric.

<details>
<summary>Code cell 7</summary>

```python
fig, ax = plt.subplots(figsize=(7, 3.6))
ax.axhline(0.10, linestyle="--", alpha=0.7, label="C=0.10 (10% RMS)")
ax.axhline(0.05, linestyle=":", alpha=0.9, label="C=0.05 (5% RMS)")
ax.plot(np.arange(len(df_scen)), df_scen["C_pred"].to_numpy(), marker="o")
ax.set_xticks(np.arange(len(df_scen)))
ax.set_xticklabels([f"C{i}" for i in range(len(df_scen))])
ax.set_ylabel("Predicted speckle contrast C")
ax.set_title("Scenario-level predicted C from spectral weights")
ax.grid(True, alpha=0.3)
ax.legend(loc="upper right")
plt.show()
```

</details>

## 5) Simulate excitation fields: component patterns + summed field

We now generate actual 2D excitation fields for a subset of scenarios.

Key point: **averaging does not change speckle grain size**.
It reduces the *amplitude* of the fluctuations.

So if a single-wavelength speckle pattern is “fine”, the averaged pattern remains “fine” but with lower contrast.
Whether that is acceptable depends on the PSF scale and your detector thresholding.

<details>
<summary>Code cell 8</summary>

```python
coords_um = (np.arange(N_grid) - N_grid // 2) * dx_um
X_um, Y_um = np.meshgrid(coords_um, coords_um, indexing="xy")

roi_mask = square_roi_mask(N_grid, dx_um, roi_um)
inner_mask = roi_mask & (np.abs(X_um) <= 0.5 * roi_um - inner_margin_um) & (np.abs(Y_um) <= 0.5 * roi_um - inner_margin_um)


def speckle_contrast_inner(I: np.ndarray) -> float:
    vals = I[inner_mask]
    return float(vals.std() / max(vals.mean(), 1e-12))


def simulate_weighted_excitation_field(
    *,
    weights: np.ndarray,
    seed0: int,
    n: int,
    dx_um: float,
    roi_um: float,
    lambda_um: float,
    na_illum: float,
) -> tuple[np.ndarray, list[np.ndarray]]:
    """Simulate weighted incoherent sum of independent speckle intensities.

    Returns (I_sum, components) where:
    - I_sum is normalized to mean(inner ROI)=1
    - components are also normalized the same way (so you can compare visually)

    Notes
    -----
    We enforce independence by using different RNG seeds per component.
    """

    w = np.asarray(weights, dtype=np.float64)
    w = w / float(w.sum())

    # Generate each component as a coherent single realization
    comps: list[np.ndarray] = []
    for k in range(len(w)):
        I_k, _ = simulate_excitation_speckle_field(
            n=n,
            dx_um=dx_um,
            roi_um=roi_um,
            lambda_um=lambda_um,
            na_illum=na_illum,
            exposure_s=1.0,      # irrelevant because scrambler_hz=0
            scrambler_hz=0.0,    # force 1 realization
            n_src=1,
            seed=seed0 + 1000 * k,
        )
        comps.append(I_k)

    # Weighted sum of intensities
    I_sum = np.zeros((n, n), dtype=np.float64)
    for wk, Ik in zip(w, comps):
        I_sum += wk * Ik

    # Normalize by inner ROI mean for easy comparison
    mean_inner = float(I_sum[inner_mask].mean())
    I_sum_n = I_sum / max(mean_inner, 1e-12)
    comps_n = [Ik / max(mean_inner, 1e-12) for Ik in comps]
    return I_sum_n, comps_n


def show_component_and_sum(
    *,
    scenario_name: str,
    I_sum: np.ndarray,
    components: list[np.ndarray],
    n_show_components: int = 3,
) -> None:
    """Visual summary: a few components + the sum + histogram."""

    n_show = min(n_show_components, len(components))
    fig = plt.figure(figsize=(12, 7), constrained_layout=True)
    gs = fig.add_gridspec(2, n_show + 2)

    # Components
    for i in range(n_show):
        ax = fig.add_subplot(gs[0, i])
        im = ax.imshow(
            components[i],
            origin="lower",
            extent=[coords_um[0], coords_um[-1], coords_um[0], coords_um[-1]],
            interpolation="nearest",
        )
        ax.set_title(f"component {i+1}")
        ax.add_patch(
            plt.Rectangle(
                (-0.5 * roi_um, -0.5 * roi_um),
                roi_um,
                roi_um,
                fill=False,
                linewidth=1.5,
                color="w",
            )
        )
        ax.set_xlabel("x [µm]")
        ax.set_ylabel("y [µm]")
        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)

    # Sum field
    axS = fig.add_subplot(gs[0, n_show])
    imS = axS.imshow(
        I_sum,
        origin="lower",
        extent=[coords_um[0], coords_um[-1], coords_um[0], coords_um[-1]],
        interpolation="nearest",
    )
    axS.add_patch(
        plt.Rectangle(
            (-0.5 * roi_um, -0.5 * roi_um),
            roi_um,
            roi_um,
            fill=False,
            linewidth=2.0,
            color="w",
        )
    )
    axS.set_title("weighted sum")
    axS.set_xlabel("x [µm]")
    axS.set_ylabel("y [µm]")
    plt.colorbar(imS, ax=axS, fraction=0.046, pad=0.04)

    # Histogram (inner ROI)
    axH = fig.add_subplot(gs[0, n_show + 1])
    vals = I_sum[inner_mask].ravel()
    axH.hist(vals, bins=80, density=True, alpha=0.8)
    axH.set_title("inner ROI PDF")
    axH.set_xlabel(r"$I/\langle I\rangle_{\rm inner}$")
    axH.set_ylabel("PDF")

    # A zoom patch centered in the ROI (to see grain)
    patch_half_um = 5.0
    patch = (
        (np.abs(X_um) <= patch_half_um) & (np.abs(Y_um) <= patch_half_um)
    )
    ys, xs = np.where(patch)
    y0, y1 = ys.min(), ys.max() + 1
    x0, x1 = xs.min(), xs.max() + 1

    axZ = fig.add_subplot(gs[1, : n_show + 1])
    imZ = axZ.imshow(
        I_sum[y0:y1, x0:x1],
        origin="lower",
        extent=[-patch_half_um, patch_half_um, -patch_half_um, patch_half_um],
        interpolation="nearest",
    )
    axZ.set_title("zoom (10 µm × 10 µm patch in center)")
    axZ.set_xlabel("x [µm]")
    axZ.set_ylabel("y [µm]")
    plt.colorbar(imZ, ax=axZ, fraction=0.046, pad=0.04)

    C_meas = speckle_contrast_inner(I_sum)
    fig.suptitle(f"{scenario_name} | NA_illum={NA_illum:.3f} | C_inner={C_meas:.3f}")
    plt.show()


# Choose a subset to visualize (full set can be slow)
scenario_names_to_visualize = {
    "C0: instantaneous single-line (worst case)",
    "C2: 20 spikes, equal power",
    "C4: 20 spikes, one dominates 50%",
    "C5: top-hat continuum over 2.5 nm (upper bound)",
}

vis_results = {}

for sc in scenarios:
    if sc.name not in scenario_names_to_visualize:
        continue

    w, neff, label = scenario_weights_and_neff(sc, lambda0_nm=lambda0_nm, corr_width_nm=dlam_c_nm)

    # For very large bin counts (e.g. top-hat continuum), generating every component separately is expensive.
    # If weights are uniform, we can generate the sum directly by averaging N independent realizations.
    uniform_weights = np.allclose(w, w[0])

    if uniform_weights and len(w) > 200:
        # Direct average using n_src = N (equal weights)
        I_sum, _meta = simulate_excitation_speckle_field(
            n=N_grid,
            dx_um=dx_um,
            roi_um=roi_um,
            lambda_um=lambda0_nm * 1e-3,
            na_illum=NA_illum,
            exposure_s=1.0,
            scrambler_hz=0.0,
            n_src=int(len(w)),
            seed=seed_vis,
        )
        I_sum = I_sum / float(I_sum[inner_mask].mean())

        # Still generate a few individual components for display
        comps = []
        for k in range(3):
            I_k, _ = simulate_excitation_speckle_field(
                n=N_grid,
                dx_um=dx_um,
                roi_um=roi_um,
                lambda_um=lambda0_nm * 1e-3,
                na_illum=NA_illum,
                exposure_s=1.0,
                scrambler_hz=0.0,
                n_src=1,
                seed=seed_vis + 1234 * (k + 1),
            )
            comps.append(I_k / float(I_sum[inner_mask].mean()))
    else:
        I_sum, comps = simulate_weighted_excitation_field(
            weights=w,
            seed0=seed_vis,
            n=N_grid,
            dx_um=dx_um,
            roi_um=roi_um,
            lambda_um=lambda0_nm * 1e-3,
            na_illum=NA_illum,
        )

    vis_results[sc.name] = {
        "weights": w,
        "N_eff": neff,
        "I_sum": I_sum,
        "components": comps,
        "C_inner": speckle_contrast_inner(I_sum),
        "label": label,
    }

    show_component_and_sum(scenario_name=sc.name, I_sum=I_sum, components=comps, n_show_components=3)
```

</details>

## 6) Scenario → (C, confusion matrix): Slice0 FP/FN proxy

The question you care about is not purely “is the field pretty?”.
It is: **does the nonuniformity cause missed spots or false positives?**

We reuse the synthetic-emitter setup from `notebooks/05_excitation_speckle_fpfn_proxy.py`:

- emitters are placed randomly (non-overlapping) inside the inner ROI
- brightness is proportional to local excitation intensity
- background can include a component proportional to excitation (to emulate structured background)
- we run the actual `Slice0` detector and compute TP/FP/FN
- TN is approximated via “decoy sites” (random points that should be empty)

We do **not** claim this is a full physical microscopy simulator.
It is a *consistent, reproducible* way to measure whether different speckle scenarios produce meaningfully
different detector behavior.

<details>
<summary>Code cell 9</summary>

```python
@dataclass(frozen=True)
class EmitterSimConfig:
    n_emitters: int = 60
    min_separation_px: int = 18
    photons_per_emitter_mean: float = 900.0
    photons_per_emitter_sigma_frac: float = 0.25
    bg_photons_flat: float = 3.0
    bg_photons_scale_exc: float = 10.0
    psf_sigma_px: float = 2.8
    psf_kernel_radius_px: int = 12


cfg = EmitterSimConfig()

# Slice0 params (tune u0_min here)
params = Slice0Params(
    pixel_size_nm=dx_um * 1e3,  # µm → nm
    spot_radius_nm=270.0,
    q_min=1.0,
    u0_min=30.0,
)

# Precompute a Gaussian PSF kernel (normalized to sum=1)
R = cfg.psf_kernel_radius_px
yy, xx = np.mgrid[-R : R + 1, -R : R + 1]
psf = np.exp(-(xx * xx + yy * yy) / (2.0 * cfg.psf_sigma_px * cfg.psf_sigma_px))
psf /= psf.sum()


def place_emitters_nonoverlap(
    *,
    n_emitters: int,
    roi_mask: np.ndarray,
    min_sep_px: int,
    rng: np.random.Generator,
) -> np.ndarray:
    ys, xs = np.where(roi_mask)
    coords = np.stack([ys, xs], axis=1)
    chosen: list[tuple[int, int]] = []

    # random order
    idx = rng.permutation(coords.shape[0])
    for j in idx:
        y, x = int(coords[j, 0]), int(coords[j, 1])
        ok = True
        for (y0, x0) in chosen:
            if (y - y0) * (y - y0) + (x - x0) * (x - x0) < min_sep_px * min_sep_px:
                ok = False
                break
        if ok:
            chosen.append((y, x))
            if len(chosen) >= n_emitters:
                break
    if len(chosen) < n_emitters:
        raise RuntimeError(f"Could only place {len(chosen)} emitters with min_sep_px={min_sep_px}")
    return np.array(chosen, dtype=int)


def simulate_sparse_emitter_frame(
    *,
    I_exc: np.ndarray,
    rng: np.random.Generator,
) -> tuple[np.ndarray, np.ndarray]:
    """Return (img_noisy, gt_yx) for one frame."""

    gt_yx = place_emitters_nonoverlap(
        n_emitters=cfg.n_emitters,
        roi_mask=inner_mask,
        min_sep_px=cfg.min_separation_px,
        rng=rng,
    )
    emit_y = gt_yx[:, 0]
    emit_x = gt_yx[:, 1]

    photons = cfg.photons_per_emitter_mean * (
        1.0 + cfg.photons_per_emitter_sigma_frac * rng.standard_normal(cfg.n_emitters)
    )
    photons = np.clip(
        photons,
        0.2 * cfg.photons_per_emitter_mean,
        5.0 * cfg.photons_per_emitter_mean,
    )

    exc_loc = I_exc[emit_y, emit_x]

    img = np.zeros((N_grid, N_grid), dtype=np.float64)

    # Background
    img += cfg.bg_photons_flat + cfg.bg_photons_scale_exc * I_exc

    # Add emitters
    for y0, x0, p0, e0 in zip(emit_y, emit_x, photons, exc_loc):
        amp = float(p0 * e0)
        y1 = y0 - R
        y2 = y0 + R + 1
        x1 = x0 - R
        x2 = x0 + R + 1
        if y1 < 0 or x1 < 0 or y2 > N_grid or x2 > N_grid:
            continue
        img[y1:y2, x1:x2] += amp * psf

    img_noisy = rng.poisson(img).astype(np.float32)
    return img_noisy, gt_yx


def eval_detections_confusion_proxy(
    *,
    df_det: pd.DataFrame,
    gt_yx: np.ndarray,
    rng: np.random.Generator,
    match_r_px: float = 3.0,
    n_decoy: int = 200,
) -> dict[str, int | float]:
    """Compute TP/FP/FN + decoy-site TN/FP proxy."""

    det_xy = df_det[["y_px", "x_px"]].to_numpy(dtype=float)
    gt_xy = gt_yx.astype(float)

    tp_det = np.zeros(det_xy.shape[0], dtype=bool)
    gt_matched = np.zeros(gt_xy.shape[0], dtype=bool)

    for i, (y, x) in enumerate(det_xy):
        if gt_xy.shape[0] == 0:
            break
        dy = gt_xy[:, 0] - y
        dx = gt_xy[:, 1] - x
        d2 = dy * dy + dx * dx
        j = int(np.argmin(d2))
        if d2[j] <= match_r_px * match_r_px and not gt_matched[j]:
            tp_det[i] = True
            gt_matched[j] = True

    TP = int(tp_det.sum())
    FP = int((~tp_det).sum())
    FN = int((~gt_matched).sum())

    precision = TP / max(TP + FP, 1)
    recall = TP / max(TP + FN, 1)

    # Finite TN proxy: decoy sites in inner ROI not near any GT emitter
    decoy_xy: list[tuple[int, int]] = []
    tries = 0
    while len(decoy_xy) < n_decoy and tries < 200000:
        tries += 1
        y = int(rng.integers(0, N_grid))
        x = int(rng.integers(0, N_grid))
        if not inner_mask[y, x]:
            continue
        # far from GT
        dy = gt_yx[:, 0] - y
        dx = gt_yx[:, 1] - x
        if np.any(dy * dy + dx * dx <= (match_r_px * 2.0) ** 2):
            continue
        decoy_xy.append((y, x))

    decoy_xy_arr = np.array(decoy_xy, dtype=float)
    TN = 0
    FP_decoy = 0
    for (y, x) in decoy_xy_arr:
        if det_xy.shape[0] == 0:
            TN += 1
            continue
        dy = det_xy[:, 0] - y
        dx = det_xy[:, 1] - x
        if np.any(dy * dy + dx * dx <= match_r_px * match_r_px):
            FP_decoy += 1
        else:
            TN += 1

    return {
        "TP": TP,
        "FP": FP,
        "FN": FN,
        "TN*": int(TN),
        "FP*": int(FP_decoy),
        "precision": float(precision),
        "recall": float(recall),
    }


def run_monte_carlo_for_field(
    *,
    I_exc: np.ndarray,
    n_trials: int,
    seed0: int,
) -> dict[str, float]:
    """Aggregate confusion metrics over many random emitter/noise realizations."""

    totals = {"TP": 0, "FP": 0, "FN": 0, "TN*": 0, "FP*": 0}
    precs: list[float] = []
    recs: list[float] = []

    for t in range(n_trials):
        rng = np.random.default_rng(seed0 + 10_000 * t)
        img, gt = simulate_sparse_emitter_frame(I_exc=I_exc, rng=rng)
        df_det = detect_spots(img, params)
        out = eval_detections_confusion_proxy(df_det=df_det, gt_yx=gt, rng=rng)

        for k in totals:
            totals[k] += int(out[k])  # type: ignore[arg-type]
        precs.append(float(out["precision"]))
        recs.append(float(out["recall"]))

    TP = totals["TP"]
    FP = totals["FP"]
    FN = totals["FN"]
    TN = totals["TN*"]
    FPd = totals["FP*"]

    precision = TP / max(TP + FP, 1)
    recall = TP / max(TP + FN, 1)
    f1 = (2 * precision * recall) / max(precision + recall, 1e-12)

    # Decoy-based specificity proxy
    specificity = TN / max(TN + FPd, 1)

    return {
        "TP": float(TP),
        "FP": float(FP),
        "FN": float(FN),
        "TN*": float(TN),
        "FP*": float(FPd),
        "precision": float(precision),
        "recall": float(recall),
        "F1": float(f1),
        "specificity*": float(specificity),
        "precision_mean": float(np.mean(precs)),
        "precision_std": float(np.std(precs)),
        "recall_mean": float(np.mean(recs)),
        "recall_std": float(np.std(recs)),
    }


# Run a modest Monte Carlo for each scenario (use fewer trials if this is slow on your machine)
mc_trials = 20

rows = []
for sc in scenarios:
    # Build (or reuse) a representative excitation field for the scenario.
    # For speed we reuse the single visualization field we already generated when available.
    if sc.name in vis_results:
        I_exc = vis_results[sc.name]["I_sum"]
    else:
        w, _neff, _label = scenario_weights_and_neff(sc, lambda0_nm=lambda0_nm, corr_width_nm=dlam_c_nm)
        uniform_weights = np.allclose(w, w[0])
        if uniform_weights:
            I_exc, _ = simulate_excitation_speckle_field(
                n=N_grid,
                dx_um=dx_um,
                roi_um=roi_um,
                lambda_um=lambda0_nm * 1e-3,
                na_illum=NA_illum,
                exposure_s=1.0,
                scrambler_hz=0.0,
                n_src=int(len(w)),
                seed=seed_vis,
            )
            I_exc = I_exc / float(I_exc[inner_mask].mean())
        else:
            I_exc, _comps = simulate_weighted_excitation_field(
                weights=w,
                seed0=seed_vis,
                n=N_grid,
                dx_um=dx_um,
                roi_um=roi_um,
                lambda_um=lambda0_nm * 1e-3,
                na_illum=NA_illum,
            )

    # Metrics
    C_inner = speckle_contrast_inner(I_exc)
    w, neff, label = scenario_weights_and_neff(sc, lambda0_nm=lambda0_nm, corr_width_nm=dlam_c_nm)
    C_pred = float(speckle_contrast_from_weights(w))

    stats = run_monte_carlo_for_field(I_exc=I_exc, n_trials=mc_trials, seed0=1_000_000)

    rows.append(
        {
            "scenario": sc.name,
            "spectrum_model": label,
            "N_eff": neff,
            "C_pred": C_pred,
            "C_inner_meas": C_inner,
            **stats,
        }
    )


df_out = pd.DataFrame(rows)

# Compact display
cols_show = [
    "scenario",
    "spectrum_model",
    "N_eff",
    "C_pred",
    "C_inner_meas",
    "precision",
    "recall",
    "F1",
    "specificity*",
    "FP",
    "FN",
]

df_out[cols_show]
```

</details>

### 6.1 Plot: scenario → (C, detection metrics)

<details>
<summary>Code cell 10</summary>

```python
fig, ax = plt.subplots(figsize=(8, 4.0))
ax.plot(df_out["C_inner_meas"], df_out["recall"], marker="o", linestyle="-", label="recall")
ax.plot(df_out["C_inner_meas"], df_out["precision"], marker="s", linestyle="-", label="precision")
ax.set_xlabel("Measured C (inner ROI)")
ax.set_ylabel("metric")
ax.set_title("Scenario-level detector metrics vs measured speckle contrast")
ax.grid(True, alpha=0.3)
ax.legend()
plt.show()
```

</details>

## 7) Speckle grain size and the underfill ratio

Your concern (paraphrased):

> “If each wavelength produces a very fine speckle pattern, then even if I add many independent patterns,
> won’t I still see the fine structure?”

Yes, you still see structure at the same correlation length — but with reduced amplitude.
The correlation length (grain size) is set mainly by $`\mathrm{NA}_{\mathrm{illum}}`$:

$$
\Delta x_{\mathrm{speckle}} \approx \frac{\lambda}{2\thinspace\mathrm{NA}_{\mathrm{illum}}}.
$$

Underfilling the BFP reduces $`\mathrm{NA}_{\mathrm{illum}}`$ and makes speckle grains larger.
That can be good (speckle becomes a slow multiplicative shading) or bad (big dark regions cause FNs).

We do a small sweep over underfill ratios for one mid-case scenario.

<details>
<summary>Code cell 11</summary>

```python
# Choose a mid-case scenario to sweep
sweep_scenario = next(sc for sc in scenarios if sc.name.startswith("C2:"))

w_sweep, neff_sweep, _ = scenario_weights_and_neff(sweep_scenario, lambda0_nm=lambda0_nm, corr_width_nm=dlam_c_nm)

underfill_list = [0.05, 0.10, 0.20, 0.30]

rows = []
for uf in underfill_list:
    na = uf * NA_obj

    # Equal-weight spikes -> can simulate by averaging len(w) patterns
    I_exc, _ = simulate_excitation_speckle_field(
        n=N_grid,
        dx_um=dx_um,
        roi_um=roi_um,
        lambda_um=lambda0_nm * 1e-3,
        na_illum=na,
        exposure_s=1.0,
        scrambler_hz=0.0,
        n_src=int(len(w_sweep)),
        seed=seed_vis,
    )
    I_exc = I_exc / float(I_exc[inner_mask].mean())

    C_meas = speckle_contrast_inner(I_exc)
    grain_um = (lambda0_nm * 1e-3) / (2.0 * na)

    stats = run_monte_carlo_for_field(I_exc=I_exc, n_trials=10, seed0=2_000_000 + int(1e4 * uf))

    rows.append(
        {
            "underfill_ratio": uf,
            "NA_illum": na,
            "speckle_grain_um (est)": grain_um,
            "C_inner_meas": C_meas,
            "precision": stats["precision"],
            "recall": stats["recall"],
            "F1": stats["F1"],
            "FP": stats["FP"],
            "FN": stats["FN"],
        }
    )

df_sweep = pd.DataFrame(rows)
df_sweep
```

</details>

<details>
<summary>Code cell 12</summary>

```python
fig, ax = plt.subplots(figsize=(7.5, 4.0))
ax.plot(df_sweep["speckle_grain_um (est)"], df_sweep["recall"], marker="o", label="recall")
ax.plot(df_sweep["speckle_grain_um (est)"], df_sweep["precision"], marker="s", label="precision")
ax.set_xlabel("Estimated speckle grain size (µm)")
ax.set_ylabel("metric")
ax.set_title("Effect of grain size (NA_illum) at fixed spectral scenario")
ax.grid(True, alpha=0.3)
ax.legend()
plt.show()
```

</details>

## 8) The 500 µs simultaneity question: what to ask the vendor

Your spectral-diversity strategy needs the spectral components to be effectively **simultaneous** within 500 µs.
“Linewidth” on a spec sheet is often measured on an optical spectrum analyzer (OSA), which integrates over time.
That can hide slow spectral wandering.

Practical, vendor-answerable questions:

1. **Is the 2–3 nm number instantaneous?**
   - Ask for an OSA trace with a stated sweep time / resolution bandwidth.
   - Ask whether the source is **Fabry–Perot multi-longitudinal-mode** (many modes at once),
     or single-frequency with drift.

2. **How many spectral peaks are present simultaneously?**
   - If they can provide a screenshot, you can literally count peaks.

3. **What is the relative power distribution across peaks?**
   - If one peak dominates, $N_{\mathrm{eff}}$ collapses.

4. **Is there any internal mode scrambling?**
   - Your quote says “after homogenizing fiber”. Confirm whether the “homogenizing” effect is purely the MMF,
     or whether there is any additional internal agitation.

With these answers you can choose which of C0–C5 is closest to reality.

## 9) Takeaways (how to interpret this notebook)

1. The fiber’s step-index modal dispersion makes $`\Delta\lambda_c`$ *very small*.
   That means a 2–3 nm wide spectrum has **room** to create a large $N_\lambda$.

2. In practice, your *actual* $N_{\mathrm{eff}}$ may be limited not by the fiber, but by the **laser’s internal spectrum**:
   - If you only have ~10–20 spikes, you are in the C1–C2 regime.
   - To reach “display-like” low speckle (C~0.05–0.1) you likely need tens to >100 effective components.
   - A continuous-ish band is an upper bound (C5).

3. Underfilling the BFP changes **grain size**, not **contrast** (for a fixed $N_{\mathrm{eff}}$).
   The “best” underfill depends on whether your limiting failure mode is FP (PSF-scale structure)
   or FN (large-scale shading).

4. The most important next step is to turn the vendor ambiguity into a concrete measurement:
   request a spectrum screenshot and ask the “simultaneous peaks?” question.
